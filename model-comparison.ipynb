{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport json\nimport transformers\nimport torch\nimport random\nfrom torch import cuda\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import BertTokenizer, AlbertTokenizer, DebertaV2Tokenizer, DistilBertTokenizer, ElectraTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-05T21:25:59.221501Z","iopub.execute_input":"2023-08-05T21:25:59.221916Z","iopub.status.idle":"2023-08-05T21:26:16.501219Z","shell.execute_reply.started":"2023-08-05T21:25:59.221872Z","shell.execute_reply":"2023-08-05T21:26:16.500162Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:16.503572Z","iopub.execute_input":"2023-08-05T21:26:16.503956Z","iopub.status.idle":"2023-08-05T21:26:16.508855Z","shell.execute_reply.started":"2023-08-05T21:26:16.503920Z","shell.execute_reply":"2023-08-05T21:26:16.507858Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"random_seed = 42\nrandom.seed(random_seed)\n\n# Set the random seed for NumPy (if used)\nnp.random.seed(random_seed)\n\n# Set the random seed for PyTorch\ntorch.manual_seed(random_seed)\n\n# Additional steps if using CUDA (GPU)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(random_seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:16.510892Z","iopub.execute_input":"2023-08-05T21:26:16.511751Z","iopub.status.idle":"2023-08-05T21:26:16.556361Z","shell.execute_reply.started":"2023-08-05T21:26:16.511704Z","shell.execute_reply":"2023-08-05T21:26:16.555227Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def index_converter(context, context_tokenized):\n    \"\"\"\n    Maps start str index to a tokenized index\n    \"\"\"\n    mapper = {}\n    curr = ''\n    token_idx = 0\n    for i, char in enumerate(context):\n        if char != ' ' and char != '\\n' and char != '\\t' and char != '\\r': # making sure current char is not whitespace\n            curr += char\n            if curr == context_tokenized[token_idx]:\n                start = i - len(curr) + 1\n                for j in range(start, i+1):\n                    mapper[j] = (curr, token_idx)                \n                curr = ''\n                token_idx += 1\n    if token_idx != len(context_tokenized): # skipping the data in case of issue with spanning\n        return None\n    return mapper\n\ndef preprocess_data(dataset, is_training=True, tokenized=True):\n    \"\"\"\n    Parse the json_data object into a pandas readable data representation (list of dicts)\n    \"\"\"\n    \n    def _tokenize(seq):\n        \"\"\"\n        Minimizes errors between tokenizers and encodings.\n        Recommended in the paper BiDAF (Seo et al., 2016)\n        \"\"\"\n        return [t.replace(\"``\", '\"').replace(\"''\", '\"') for t in seq.split()]\n    \n    examples = [] # store rows of data here for qa\n    \n    tokenization_errors = 0\n    misaligned_ans_errors = 0\n    num_impossibles = 0\n    num_questions = 0\n    \n    for article_id in tqdm(range(len(dataset['data']))): # for each context\n        paragraphs = dataset['data'][article_id]['paragraphs']\n        for paragraph_id in range(len(paragraphs)):\n            questions = dataset['data'][article_id]['paragraphs'][paragraph_id]['qas']\n            \n            context = paragraphs[paragraph_id]['context']\n            context_tokenized = _tokenize(context)\n                    \n            for qid in range(len(questions)): # loop through questions\n                num_questions += 1\n                \n                question = questions[qid]['question']\n                question_tokenized = _tokenize(question)\n                qas_id = questions[qid]['id']\n                \n                is_impossible = questions[qid]['is_impossible']\n                \n                if is_impossible: # check if question is impossible to answer\n                    num_impossibles += 1\n                    examples.append({'qas_id': qas_id, \n                                     'question':question_tokenized if tokenized else question, \n                                     'context': context_tokenized if tokenized else context, \n                                     'answer':'', \n                                     'is_impossible': is_impossible,\n                                     'start_pos': -1, \n                                     'end_pos':-1,\n                                    'santiy_check': context_tokenized[-1:0]})\n                    continue\n                    \n                # question is not impossible, continue parsing\n                answers = questions[qid]['answers']\n                \n                for ans_id in range(len(answers)): # for each answer\n                    answer = answers[ans_id]['text']\n                    start_pos = answers[ans_id]['answer_start'] # inclusive start index in raw context\n                    end_pos = start_pos + len(answer) #exclusive end index in raw context\n                          \n                    if context[start_pos:end_pos] != answer:\n                        misaligned_ans_errors += 1\n                        continue\n                        \n                    if tokenized:\n                        mapper = index_converter(context, context_tokenized)\n                        if mapper is None:\n                            tokenization_errors += 1\n                            continue\n                        \n                        start_pos = mapper[start_pos][1]\n                        end_pos = mapper[end_pos-1][1] # inclusive\n                    \n                    examples.append({'qas_id': qas_id, \n                                     'question':question_tokenized if tokenized else question, \n                                     'context': context_tokenized if tokenized else context, \n                                     'answer':answer, \n                                     'is_impossible': is_impossible,\n                                     'start_pos': start_pos, \n                                     'end_pos':end_pos,\n                                    'santiy_check': context_tokenized[start_pos:end_pos+1] if tokenized else context[start_pos:end_pos+1]})\n            \n                    \n    print('No. of questions:{}'.format(num_questions))\n    return examples","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:16.559430Z","iopub.execute_input":"2023-08-05T21:26:16.560460Z","iopub.status.idle":"2023-08-05T21:26:16.582126Z","shell.execute_reply.started":"2023-08-05T21:26:16.560419Z","shell.execute_reply":"2023-08-05T21:26:16.580688Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Loading the dev file","metadata":{}},{"cell_type":"code","source":"# Loading validation data\nval_file = open('/kaggle/input/squad-2/dev-v2.0.json')\nval_data = json.load(val_file)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:16.584203Z","iopub.execute_input":"2023-08-05T21:26:16.586404Z","iopub.status.idle":"2023-08-05T21:26:16.701970Z","shell.execute_reply.started":"2023-08-05T21:26:16.586364Z","shell.execute_reply":"2023-08-05T21:26:16.700968Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"val_processed = preprocess_data(val_data)\nrandom.shuffle(val_processed)\nval_df = pd.DataFrame(val_processed)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:16.703701Z","iopub.execute_input":"2023-08-05T21:26:16.704298Z","iopub.status.idle":"2023-08-05T21:26:27.498642Z","shell.execute_reply.started":"2023-08-05T21:26:16.704236Z","shell.execute_reply":"2023-08-05T21:26:27.497555Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 35/35 [00:10<00:00,  3.28it/s]","output_type":"stream"},{"name":"stdout","text":"No. of questions:11873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Displaying few extracted samples","metadata":{}},{"cell_type":"code","source":"val_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.500406Z","iopub.execute_input":"2023-08-05T21:26:27.501158Z","iopub.status.idle":"2023-08-05T21:26:27.534123Z","shell.execute_reply.started":"2023-08-05T21:26:27.501122Z","shell.execute_reply":"2023-08-05T21:26:27.532703Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                     qas_id  \\\n0  5737958b1c456719005744c4   \n1  572757bef1498d1400e8f694   \n2  5733fc6ed058e614000b6711   \n3  5a67b94bf038b7001ab0c444   \n4  57308ddc396df919000961a4   \n\n                                            question  \\\n0    [What, actually, causes, rigidity, in, matter?]   \n1  [School, desegregation, in, the, United, State...   \n2  [How, much, gun, powder, was, destroyed, in, a...   \n3  [How, many, Swedish, students, were, enrolled,...   \n4  [Imperialism, is, most, often, associated, wit...   \n\n                                             context  \\\n0  [It, is, a, common, misconception, to, ascribe...   \n1  [In, many, parts, of, the, United, States,, af...   \n2  [Governor, Vaudreuil,, who, harboured, ambitio...   \n3  [In, Sweden,, pupils, are, free, to, choose, a...   \n4  [The, principles, of, imperialism, are, often,...   \n\n                          answer  is_impossible  start_pos  end_pos  \\\n0  the Pauli exclusion principle          False         33       36   \n1               African-American          False         65       65   \n2                  45,000 pounds          False         72       73   \n3                                          True         -1       -1   \n4             the British Empire          False         13       15   \n\n                                   santiy_check  \n0  [the, Pauli, exclusion, principle.[citation]  \n1                            [African-American]  \n2                              [45,000, pounds]  \n3                                            []  \n4                        [the, British, Empire]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5737958b1c456719005744c4</td>\n      <td>[What, actually, causes, rigidity, in, matter?]</td>\n      <td>[It, is, a, common, misconception, to, ascribe...</td>\n      <td>the Pauli exclusion principle</td>\n      <td>False</td>\n      <td>33</td>\n      <td>36</td>\n      <td>[the, Pauli, exclusion, principle.[citation]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>572757bef1498d1400e8f694</td>\n      <td>[School, desegregation, in, the, United, State...</td>\n      <td>[In, many, parts, of, the, United, States,, af...</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>65</td>\n      <td>65</td>\n      <td>[African-American]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733fc6ed058e614000b6711</td>\n      <td>[How, much, gun, powder, was, destroyed, in, a...</td>\n      <td>[Governor, Vaudreuil,, who, harboured, ambitio...</td>\n      <td>45,000 pounds</td>\n      <td>False</td>\n      <td>72</td>\n      <td>73</td>\n      <td>[45,000, pounds]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5a67b94bf038b7001ab0c444</td>\n      <td>[How, many, Swedish, students, were, enrolled,...</td>\n      <td>[In, Sweden,, pupils, are, free, to, choose, a...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57308ddc396df919000961a4</td>\n      <td>[Imperialism, is, most, often, associated, wit...</td>\n      <td>[The, principles, of, imperialism, are, often,...</td>\n      <td>the British Empire</td>\n      <td>False</td>\n      <td>13</td>\n      <td>15</td>\n      <td>[the, British, Empire]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Functions for Calculation of Evaluation Metrics\n\ndef normalize_text(s):\n    \"\"\"Removing articles, punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(prediction, truth):\n    ''' Calculates the F1 score'''\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n\n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n\n    common_tokens = set(pred_tokens) & set(truth_tokens)\n\n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n\n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n\n    return 2 * (prec * rec) / (prec + rec)\n\ndef compute_exact_match(prediction, truth):\n    ''' Computes the exact match score '''\n    return int(normalize_text(prediction) == normalize_text(truth))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.756418Z","iopub.execute_input":"2023-08-05T21:26:27.757516Z","iopub.status.idle":"2023-08-05T21:26:27.771530Z","shell.execute_reply.started":"2023-08-05T21:26:27.757475Z","shell.execute_reply":"2023-08-05T21:26:27.770340Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def encode_data(processed_data, tokenizer, max_len, max_query_len):\n    \"\"\"\n    Converts examples of data into input format tensors based on the tokenizer\n    \"\"\"\n    context_length_errors = 0\n    encoded_data = []\n    for sample in tqdm(processed_data):\n        question_raw = ' '.join(sample['question'])\n        context_raw = ' '.join(sample['context'])\n        if len(question_raw) > max_query_len:\n            question_raw = question_raw[:max_query_len]\n        \n        # encode the data using the tokenizer\n        encoded = tokenizer.encode_plus(question_raw, context_raw,\n                                        max_length=max_len,\n                                        padding='max_length',\n                                        truncation='only_second',\n                                        return_token_type_ids=True)\n        if sample['is_impossible']:\n            start = -1\n            end = -1\n        else: # Adjust the start_pos and end_pos \n            input_ids = encoded['input_ids']\n            answer_ids = tokenizer.encode(sample['answer']) # get token ids for answer to compare\n            start, end = 0, 0 # defaults to this, if encode_plus performed truncation which included answer\n            for i in range(len(input_ids)):\n                if input_ids[i: i+len(answer_ids[1:-1])] == answer_ids[1:-1]:\n                    start = i\n                    end = i + len(answer_ids[1:-1]) - 1\n                    break\n            \n        ids = encoded['input_ids']\n        token_type_ids = encoded['token_type_ids']\n        mask = encoded['attention_mask']\n        \n        assert len(ids) == max_len\n        assert len(token_type_ids) == max_len\n        assert len(mask) == max_len\n        \n        encoded_data.append({'ids': ids,\n                      'token_type_ids': token_type_ids,\n                      'mask': mask,\n                      'start_pos': start,\n                      'end_pos': end})        \n    return encoded_data","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.773347Z","iopub.execute_input":"2023-08-05T21:26:27.774116Z","iopub.status.idle":"2023-08-05T21:26:27.788971Z","shell.execute_reply.started":"2023-08-05T21:26:27.774070Z","shell.execute_reply":"2023-08-05T21:26:27.787795Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, testing_loader):\n    ''' Predicts the answers for the model and tokenizer on the loader provided'''\n    model.eval()\n    preds = []\n    targs = []\n    pred_answers = []\n    target_answers = []\n    with torch.no_grad():\n        for data in tqdm(testing_loader):\n            data = tuple(d.to(device) for d in data)\n            inputs = {'input_ids': data[0],\n                'attention_mask':  data[1], \n                'token_type_ids':  data[2],\n                'start_positions': data[3], \n                'end_positions':   data[4]}\n            output = model(**inputs)\n            starts = output[1]\n            ends = output[2]\n            start_preds = []\n            end_preds = []\n            \n            target_starts = data[3]\n            target_ends = data[4]\n\n            for s,e in zip(starts, ends):\n                start_pred = torch.argmax(s)\n                start_preds.append(start_pred)\n                end_pred = torch.argmax(e)\n                end_preds.append(end_pred)\n\n            for i, (s,e) in enumerate(zip(start_preds, target_ends)):\n                preds.append((s,e))\n                predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                pred_answers.append(predicted_answer)\n            \n            for i, (s,e) in enumerate(zip(target_starts, target_ends)):\n                targs.append((s,e))\n                actual_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                target_answers.append(actual_answer)\n\n        pred_answers = [item if item != '[CLS]' else '' for item in pred_answers]\n    return pred_answers, target_answers","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.790540Z","iopub.execute_input":"2023-08-05T21:26:27.791222Z","iopub.status.idle":"2023-08-05T21:26:27.806492Z","shell.execute_reply.started":"2023-08-05T21:26:27.791178Z","shell.execute_reply":"2023-08-05T21:26:27.805280Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def create_dataloader(encodings):\n    ''' Creation of dataloader for the encodings'''\n    input_ids = torch.tensor([sample['ids'] for sample in encodings], dtype=torch.long)\n    input_mask = torch.tensor([sample['mask'] for sample in encodings], dtype=torch.long)\n    segment_ids = torch.tensor([sample['token_type_ids'] for sample in encodings], dtype=torch.long)\n    start_positions = torch.tensor([sample['start_pos'] for sample in encodings], dtype=torch.long)\n    end_positions = torch.tensor([sample['end_pos'] for sample in encodings], dtype=torch.long)\n    dataset = TensorDataset(input_ids, input_mask, segment_ids, start_positions, end_positions)\n    params = {'batch_size': BATCH_SIZE,\n                    'shuffle': False,\n                    'num_workers': 0\n                    }    \n    data_loader = DataLoader(dataset, **params)\n    return data_loader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model parameters\nMAX_SEQ_LEN = 512\nMAX_QN_LEN = 128\nBATCH_SIZE = 8","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.829714Z","iopub.execute_input":"2023-08-05T21:26:27.830544Z","iopub.status.idle":"2023-08-05T21:26:27.835904Z","shell.execute_reply.started":"2023-08-05T21:26:27.830499Z","shell.execute_reply":"2023-08-05T21:26:27.834808Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.837441Z","iopub.execute_input":"2023-08-05T21:26:27.838461Z","iopub.status.idle":"2023-08-05T21:26:27.847788Z","shell.execute_reply.started":"2023-08-05T21:26:27.838419Z","shell.execute_reply":"2023-08-05T21:26:27.846631Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Loading finetuned BERT model and pre-trained tokenizer","metadata":{}},{"cell_type":"code","source":"bert_model = torch.load('/kaggle/input/squad2-models/fine_tuned_bert2-2.model')\nbert_model.to(device)\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:27.849706Z","iopub.execute_input":"2023-08-05T21:26:27.850885Z","iopub.status.idle":"2023-08-05T21:26:37.464084Z","shell.execute_reply.started":"2023-08-05T21:26:27.850844Z","shell.execute_reply":"2023-08-05T21:26:37.463083Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a03a4bff7c1482a892d6563ea277905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ebf8256b5c4a8e99d90bca24cb277e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656feff5d1474b109ba16e097548fc43"}},"metadata":{}}]},{"cell_type":"code","source":"# Creating encodings for BERT\nbert_encodings = encode_data(val_processed, bert_tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)\nbert_dataloader = create_dataloader(bert_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:26:37.465606Z","iopub.execute_input":"2023-08-05T21:26:37.466580Z","iopub.status.idle":"2023-08-05T21:29:33.920251Z","shell.execute_reply.started":"2023-08-05T21:26:37.466545Z","shell.execute_reply":"2023-08-05T21:29:33.918596Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [02:56<00:00, 148.67it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing predictions for BERT\nbert_predictions, actual_answer = predict(bert_model, bert_tokenizer, bert_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:29:40.434292Z","iopub.execute_input":"2023-08-05T21:29:40.434724Z","iopub.status.idle":"2023-08-05T21:37:18.721504Z","shell.execute_reply.started":"2023-08-05T21:29:40.434687Z","shell.execute_reply":"2023-08-05T21:37:18.720403Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 3279/3279 [07:38<00:00,  7.16it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading finetuned ALBERT model and pre-trained tokenizer","metadata":{}},{"cell_type":"code","source":"albert_model = torch.load('/kaggle/input/squad2-models/fine_tuned_albert2.model')\nalbert_model.to(device)\nalbert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:37:18.723894Z","iopub.execute_input":"2023-08-05T21:37:18.724911Z","iopub.status.idle":"2023-08-05T21:37:19.911711Z","shell.execute_reply.started":"2023-08-05T21:37:18.724868Z","shell.execute_reply":"2023-08-05T21:37:19.910608Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113648c47eae41d9be8fa53e74006127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deda3cfab2d1455c8aed41f6b55bba7c"}},"metadata":{}}]},{"cell_type":"code","source":"# Creating encodings for ALBERT\nalbert_encodings = encode_data(val_processed, albert_tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)\nalbert_dataloader = create_dataloader(albert_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:37:19.913377Z","iopub.execute_input":"2023-08-05T21:37:19.914035Z","iopub.status.idle":"2023-08-05T21:38:43.726581Z","shell.execute_reply.started":"2023-08-05T21:37:19.913990Z","shell.execute_reply":"2023-08-05T21:38:43.725478Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [01:17<00:00, 337.35it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing predictions for ALBERT\nalbert_predictions, actual_answer = predict(albert_model, albert_tokenizer, albert_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:38:43.736027Z","iopub.execute_input":"2023-08-05T21:38:43.736399Z","iopub.status.idle":"2023-08-05T21:47:32.024220Z","shell.execute_reply.started":"2023-08-05T21:38:43.736369Z","shell.execute_reply":"2023-08-05T21:47:32.023216Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 3279/3279 [08:48<00:00,  6.21it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading finetuned DEBERTA model and pre-trained tokenizer","metadata":{}},{"cell_type":"code","source":"deberta_model = torch.load('/kaggle/input/squad2-models/fine_tuned_deberta1.model')\ndeberta_model.to(device)\ndeberta_tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:47:32.029835Z","iopub.execute_input":"2023-08-05T21:47:32.032155Z","iopub.status.idle":"2023-08-05T21:47:39.339558Z","shell.execute_reply.started":"2023-08-05T21:47:32.032119Z","shell.execute_reply":"2023-08-05T21:47:39.338451Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197577ba88d8406f8094a091fa42371f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7450eac89c0d47bf97ee51725c40766c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9137472a8c4321ae3f5d4282b33330"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating encodings for DEBERTA\ndeberta_encodings = encode_data(val_processed, deberta_tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)\ndeberta_dataloader = create_dataloader(deberta_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:47:39.341207Z","iopub.execute_input":"2023-08-05T21:47:39.341783Z","iopub.status.idle":"2023-08-05T21:48:29.637611Z","shell.execute_reply.started":"2023-08-05T21:47:39.341734Z","shell.execute_reply":"2023-08-05T21:48:29.636484Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [00:44<00:00, 591.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing predictions for DEBERTA\ndeberta_predictions, actual_answer = predict(deberta_model, deberta_tokenizer, deberta_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T21:48:29.639632Z","iopub.execute_input":"2023-08-05T21:48:29.640091Z","iopub.status.idle":"2023-08-05T22:00:20.477990Z","shell.execute_reply.started":"2023-08-05T21:48:29.640048Z","shell.execute_reply":"2023-08-05T22:00:20.475919Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|██████████| 3279/3279 [11:50<00:00,  4.61it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading finetuned DISTILBERT model and pre-trained tokenizer","metadata":{}},{"cell_type":"code","source":"distilbert_model = torch.load('/kaggle/input/squad2-models/fine_tuned_distilbert2.model')\ndistilbert_model.to(device)\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:00:20.480187Z","iopub.execute_input":"2023-08-05T22:00:20.480551Z","iopub.status.idle":"2023-08-05T22:00:23.303574Z","shell.execute_reply.started":"2023-08-05T22:00:20.480520Z","shell.execute_reply":"2023-08-05T22:00:23.302554Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c966e1dc3f64410f8f1881386bd921a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d10e7850854da593e8c6903b2f4bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8297eb65d1744475b1711183691e2815"}},"metadata":{}}]},{"cell_type":"code","source":"def predict_distilbert(model, tokenizer, testing_loader):\n    model.eval()\n    preds = []\n    targs = []\n    pred_answers = []\n    target_answers = []\n    with torch.no_grad():\n        for data in tqdm(testing_loader):\n            data = tuple(d.to(device) for d in data)\n            inputs = {'input_ids': data[0],\n                'attention_mask':  data[1], \n                'start_positions': data[3], \n                'end_positions':   data[4]}\n            output = model(**inputs)\n            starts = output[1]\n            ends = output[2]\n            start_preds = []\n            end_preds = []\n            \n            target_starts = data[3]\n            target_ends = data[4]\n\n            for s,e in zip(starts, ends):\n                start_pred = torch.argmax(s)\n                start_preds.append(start_pred)\n                end_pred = torch.argmax(e)\n                end_preds.append(end_pred)\n\n            for i, (s,e) in enumerate(zip(start_preds, target_ends)):\n                preds.append((s,e))\n                predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                pred_answers.append(predicted_answer)\n            \n            for i, (s,e) in enumerate(zip(target_starts, target_ends)):\n                targs.append((s,e))\n                actual_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                target_answers.append(actual_answer)\n\n        pred_answers = [item if item != '[CLS]' else '' for item in pred_answers]\n    return pred_answers, target_answers","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:00:23.305234Z","iopub.execute_input":"2023-08-05T22:00:23.305621Z","iopub.status.idle":"2023-08-05T22:00:23.317983Z","shell.execute_reply.started":"2023-08-05T22:00:23.305585Z","shell.execute_reply":"2023-08-05T22:00:23.316575Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Creating encodings for DISTILBERT\ndistilbert_encodings = encode_data(val_processed, distilbert_tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)\ndistilbert_dataloader = create_dataloader(distilbert_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:00:23.319902Z","iopub.execute_input":"2023-08-05T22:00:23.320713Z","iopub.status.idle":"2023-08-05T22:03:28.083373Z","shell.execute_reply.started":"2023-08-05T22:00:23.320671Z","shell.execute_reply":"2023-08-05T22:03:28.082325Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [02:58<00:00, 147.14it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing predictions for DISTILBERT\ndistilbert_predictions, actual_answer = predict_distilbert(distilbert_model,distilbert_tokenizer, distilbert_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:03:28.084846Z","iopub.execute_input":"2023-08-05T22:03:28.085349Z","iopub.status.idle":"2023-08-05T22:07:24.733063Z","shell.execute_reply.started":"2023-08-05T22:03:28.085310Z","shell.execute_reply":"2023-08-05T22:07:24.732035Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"100%|██████████| 3279/3279 [03:56<00:00, 13.87it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading finetuned ELECTRA model and pre-trained tokenizer","metadata":{}},{"cell_type":"code","source":"electra_model = torch.load('/kaggle/input/squad2-models/fine_tuned_electra2.model')\nelectra_model.to(device)\nelectra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:07:24.735787Z","iopub.execute_input":"2023-08-05T22:07:24.736443Z","iopub.status.idle":"2023-08-05T22:07:29.066758Z","shell.execute_reply.started":"2023-08-05T22:07:24.736401Z","shell.execute_reply":"2023-08-05T22:07:29.065688Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"889a881ea1e341eb90b6f434007e87f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a93f97b85524cc69b8f6326a368e50b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3f7cf41605426089a3bc29b73017bb"}},"metadata":{}}]},{"cell_type":"code","source":"# Creating encodings for ELECTRA\nelectra_encodings = encode_data(val_processed, electra_tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)\nelectra_dataloader = create_dataloader(electra_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:07:29.068139Z","iopub.execute_input":"2023-08-05T22:07:29.069365Z","iopub.status.idle":"2023-08-05T22:10:39.397954Z","shell.execute_reply.started":"2023-08-05T22:07:29.069322Z","shell.execute_reply":"2023-08-05T22:10:39.396805Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [03:04<00:00, 142.42it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Performing predictions for ELECTRA\nelectra_predictions, actual_answer = predict(electra_model,electra_tokenizer, electra_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:10:39.399363Z","iopub.execute_input":"2023-08-05T22:10:39.399744Z","iopub.status.idle":"2023-08-05T22:18:12.246639Z","shell.execute_reply.started":"2023-08-05T22:10:39.399709Z","shell.execute_reply":"2023-08-05T22:18:12.245634Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"100%|██████████| 3279/3279 [07:32<00:00,  7.24it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Adding predictions in the dataframe\nval_df['bert_predictions'] = bert_predictions\nval_df['albert_predictions'] = albert_predictions\nval_df['deberta_predictions'] = deberta_predictions\nval_df['distilbert_predictions'] = distilbert_predictions\nval_df['electra_predictions'] = electra_predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.249050Z","iopub.execute_input":"2023-08-05T22:18:12.249499Z","iopub.status.idle":"2023-08-05T22:18:12.272877Z","shell.execute_reply.started":"2023-08-05T22:18:12.249462Z","shell.execute_reply":"2023-08-05T22:18:12.271737Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"val_has_ans = val_df[~val_df['is_impossible']]","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.274290Z","iopub.execute_input":"2023-08-05T22:18:12.275356Z","iopub.status.idle":"2023-08-05T22:18:12.312295Z","shell.execute_reply.started":"2023-08-05T22:18:12.275313Z","shell.execute_reply":"2023-08-05T22:18:12.311304Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"val_no_ans = val_df[val_df['is_impossible']]","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.314146Z","iopub.execute_input":"2023-08-05T22:18:12.314899Z","iopub.status.idle":"2023-08-05T22:18:12.323410Z","shell.execute_reply.started":"2023-08-05T22:18:12.314858Z","shell.execute_reply":"2023-08-05T22:18:12.322352Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def categorize_question_type(question):\n    question = list(map(str.lower, question))\n    if 'what' in question:\n        return 'What'\n    elif 'who' in question:\n        return 'Who'\n    elif 'when' in question:\n        return 'When'\n    elif 'where' in question:\n        return 'Where'\n    elif 'why' in question:\n        return 'Why'\n    elif 'how' in question:\n        return 'How'\n    else:\n        return 'Other'","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.325174Z","iopub.execute_input":"2023-08-05T22:18:12.325606Z","iopub.status.idle":"2023-08-05T22:18:12.333855Z","shell.execute_reply.started":"2023-08-05T22:18:12.325566Z","shell.execute_reply":"2023-08-05T22:18:12.332599Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"val_df['question_type'] = val_df['question'].apply(categorize_question_type)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.335911Z","iopub.execute_input":"2023-08-05T22:18:12.336399Z","iopub.status.idle":"2023-08-05T22:18:12.412191Z","shell.execute_reply.started":"2023-08-05T22:18:12.336360Z","shell.execute_reply":"2023-08-05T22:18:12.411125Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"val_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.413937Z","iopub.execute_input":"2023-08-05T22:18:12.414362Z","iopub.status.idle":"2023-08-05T22:18:12.461025Z","shell.execute_reply.started":"2023-08-05T22:18:12.414324Z","shell.execute_reply":"2023-08-05T22:18:12.459964Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                         qas_id  \\\n0      5737958b1c456719005744c4   \n1      572757bef1498d1400e8f694   \n2      5733fc6ed058e614000b6711   \n3      5a67b94bf038b7001ab0c444   \n4      57308ddc396df919000961a4   \n...                         ...   \n26227  5725d662ec44d21400f3d68b   \n26228  5733d7cbd058e614000b63ae   \n26229  56e1c2eee3433e1400423138   \n26230  57111b95a58dae1900cd6c51   \n26231  572fcb6da23a5019007fc9f1   \n\n                                                question  \\\n0        [What, actually, causes, rigidity, in, matter?]   \n1      [School, desegregation, in, the, United, State...   \n2      [How, much, gun, powder, was, destroyed, in, a...   \n3      [How, many, Swedish, students, were, enrolled,...   \n4      [Imperialism, is, most, often, associated, wit...   \n...                                                  ...   \n26227  [Which, park, hosts, the, largest, Civil, War,...   \n26228      [What, rule, did, some, native, live, under?]   \n26229  [Decision, problems, capable, of, being, solve...   \n26230  [What, German, poet, was, descended, from, Hug...   \n26231  [Where, are, reserved, matters, stated, in, th...   \n\n                                                 context  \\\n0      [It, is, a, common, misconception, to, ascribe...   \n1      [In, many, parts, of, the, United, States,, af...   \n2      [Governor, Vaudreuil,, who, harboured, ambitio...   \n3      [In, Sweden,, pupils, are, free, to, choose, a...   \n4      [The, principles, of, imperialism, are, often,...   \n...                                                  ...   \n26227  [Fresno, has, three, large, public, parks,, tw...   \n26228  [In, between, the, French, and, the, British,,...   \n26229  [But, bounding, the, computation, time, above,...   \n26230  [Frederick, William,, Elector, of, Brandenburg...   \n26231  [The, specific, devolved, matters, are, all, s...   \n\n                              answer  is_impossible  start_pos  end_pos  \\\n0      the Pauli exclusion principle          False         33       36   \n1                   African-American          False         65       65   \n2                      45,000 pounds          False         72       73   \n3                                              True         -1       -1   \n4                 the British Empire          False         13       15   \n...                              ...            ...        ...      ...   \n26227                   Kearney Park          False         65       66   \n26228                       Iroquois          False         90       90   \n26229             complexity class P          False         93       95   \n26230                Theodor Fontane          False         38       39   \n26231                     Schedule 5          False         13       14   \n\n                                       santiy_check  \\\n0      [the, Pauli, exclusion, principle.[citation]   \n1                                [African-American]   \n2                                  [45,000, pounds]   \n3                                                []   \n4                            [the, British, Empire]   \n...                                             ...   \n26227                               [Kearney, Park]   \n26228                                    [Iroquois]   \n26229                       [complexity, class, P,]   \n26230                           [Theodor, Fontane,]   \n26231                                 [Schedule, 5]   \n\n                                        bert_predictions  \\\n0                              pauli exclusion principle   \n1                                     african - american   \n2                                        45 , 000 pounds   \n3                                                          \n4                                     the british empire   \n...                                                  ...   \n26227                                       kearney park   \n26228                                           iroquois   \n26229  [CLS] decision problems capable of being solve...   \n26230                                                      \n26231                                         schedule 5   \n\n                                      albert_predictions  \\\n0      [CLS] what actually causes rigidity in matter?...   \n1                                       african-american   \n2                                          45,000 pounds   \n3                                                          \n4                                         british empire   \n...                                                  ...   \n26227                                       kearney park   \n26228                                                      \n26229  [CLS] decision problems capable of being solve...   \n26230                                    theodor fontane   \n26231  [CLS] where are reserved matters stated in the...   \n\n                                     deberta_predictions  \\\n0                              Pauli exclusion principle   \n1                                       African-American   \n2                                          45,000 pounds   \n3                                                          \n4                                         British Empire   \n...                                                  ...   \n26227                                       Kearney Park   \n26228                                           Iroquois   \n26229  polynomial time belon[SEP]But bounding the com...   \n26230                                    Theodor Fontane   \n26231                                         Schedule 5   \n\n          distilbert_predictions        electra_predictions question_type  \n0      pauli exclusion principle  pauli exclusion principle          What  \n1             african - american         african - american          What  \n2                45 , 000 pounds            45 , 000 pounds           How  \n3                                                                     How  \n4                 british empire             british empire         Other  \n...                          ...                        ...           ...  \n26227               kearney park               kearney park         Other  \n26228                                                                What  \n26229         complexity class p         complexity class p          What  \n26230            theodor fontane            theodor fontane          What  \n26231                 schedule 5                 schedule 5         Where  \n\n[26232 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n      <th>bert_predictions</th>\n      <th>albert_predictions</th>\n      <th>deberta_predictions</th>\n      <th>distilbert_predictions</th>\n      <th>electra_predictions</th>\n      <th>question_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5737958b1c456719005744c4</td>\n      <td>[What, actually, causes, rigidity, in, matter?]</td>\n      <td>[It, is, a, common, misconception, to, ascribe...</td>\n      <td>the Pauli exclusion principle</td>\n      <td>False</td>\n      <td>33</td>\n      <td>36</td>\n      <td>[the, Pauli, exclusion, principle.[citation]</td>\n      <td>pauli exclusion principle</td>\n      <td>[CLS] what actually causes rigidity in matter?...</td>\n      <td>Pauli exclusion principle</td>\n      <td>pauli exclusion principle</td>\n      <td>pauli exclusion principle</td>\n      <td>What</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>572757bef1498d1400e8f694</td>\n      <td>[School, desegregation, in, the, United, State...</td>\n      <td>[In, many, parts, of, the, United, States,, af...</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>65</td>\n      <td>65</td>\n      <td>[African-American]</td>\n      <td>african - american</td>\n      <td>african-american</td>\n      <td>African-American</td>\n      <td>african - american</td>\n      <td>african - american</td>\n      <td>What</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733fc6ed058e614000b6711</td>\n      <td>[How, much, gun, powder, was, destroyed, in, a...</td>\n      <td>[Governor, Vaudreuil,, who, harboured, ambitio...</td>\n      <td>45,000 pounds</td>\n      <td>False</td>\n      <td>72</td>\n      <td>73</td>\n      <td>[45,000, pounds]</td>\n      <td>45 , 000 pounds</td>\n      <td>45,000 pounds</td>\n      <td>45,000 pounds</td>\n      <td>45 , 000 pounds</td>\n      <td>45 , 000 pounds</td>\n      <td>How</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5a67b94bf038b7001ab0c444</td>\n      <td>[How, many, Swedish, students, were, enrolled,...</td>\n      <td>[In, Sweden,, pupils, are, free, to, choose, a...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>How</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57308ddc396df919000961a4</td>\n      <td>[Imperialism, is, most, often, associated, wit...</td>\n      <td>[The, principles, of, imperialism, are, often,...</td>\n      <td>the British Empire</td>\n      <td>False</td>\n      <td>13</td>\n      <td>15</td>\n      <td>[the, British, Empire]</td>\n      <td>the british empire</td>\n      <td>british empire</td>\n      <td>British Empire</td>\n      <td>british empire</td>\n      <td>british empire</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26227</th>\n      <td>5725d662ec44d21400f3d68b</td>\n      <td>[Which, park, hosts, the, largest, Civil, War,...</td>\n      <td>[Fresno, has, three, large, public, parks,, tw...</td>\n      <td>Kearney Park</td>\n      <td>False</td>\n      <td>65</td>\n      <td>66</td>\n      <td>[Kearney, Park]</td>\n      <td>kearney park</td>\n      <td>kearney park</td>\n      <td>Kearney Park</td>\n      <td>kearney park</td>\n      <td>kearney park</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>26228</th>\n      <td>5733d7cbd058e614000b63ae</td>\n      <td>[What, rule, did, some, native, live, under?]</td>\n      <td>[In, between, the, French, and, the, British,,...</td>\n      <td>Iroquois</td>\n      <td>False</td>\n      <td>90</td>\n      <td>90</td>\n      <td>[Iroquois]</td>\n      <td>iroquois</td>\n      <td></td>\n      <td>Iroquois</td>\n      <td></td>\n      <td></td>\n      <td>What</td>\n    </tr>\n    <tr>\n      <th>26229</th>\n      <td>56e1c2eee3433e1400423138</td>\n      <td>[Decision, problems, capable, of, being, solve...</td>\n      <td>[But, bounding, the, computation, time, above,...</td>\n      <td>complexity class P</td>\n      <td>False</td>\n      <td>93</td>\n      <td>95</td>\n      <td>[complexity, class, P,]</td>\n      <td>[CLS] decision problems capable of being solve...</td>\n      <td>[CLS] decision problems capable of being solve...</td>\n      <td>polynomial time belon[SEP]But bounding the com...</td>\n      <td>complexity class p</td>\n      <td>complexity class p</td>\n      <td>What</td>\n    </tr>\n    <tr>\n      <th>26230</th>\n      <td>57111b95a58dae1900cd6c51</td>\n      <td>[What, German, poet, was, descended, from, Hug...</td>\n      <td>[Frederick, William,, Elector, of, Brandenburg...</td>\n      <td>Theodor Fontane</td>\n      <td>False</td>\n      <td>38</td>\n      <td>39</td>\n      <td>[Theodor, Fontane,]</td>\n      <td></td>\n      <td>theodor fontane</td>\n      <td>Theodor Fontane</td>\n      <td>theodor fontane</td>\n      <td>theodor fontane</td>\n      <td>What</td>\n    </tr>\n    <tr>\n      <th>26231</th>\n      <td>572fcb6da23a5019007fc9f1</td>\n      <td>[Where, are, reserved, matters, stated, in, th...</td>\n      <td>[The, specific, devolved, matters, are, all, s...</td>\n      <td>Schedule 5</td>\n      <td>False</td>\n      <td>13</td>\n      <td>14</td>\n      <td>[Schedule, 5]</td>\n      <td>schedule 5</td>\n      <td>[CLS] where are reserved matters stated in the...</td>\n      <td>Schedule 5</td>\n      <td>schedule 5</td>\n      <td>schedule 5</td>\n      <td>Where</td>\n    </tr>\n  </tbody>\n</table>\n<p>26232 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"models = ['Bert','Albert', 'DeBerta', 'DistilBert', 'Electra']\npredictions = ['bert_predictions', 'albert_predictions', 'deberta_predictions', 'distilbert_predictions', 'electra_predictions']","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.462825Z","iopub.execute_input":"2023-08-05T22:18:12.463539Z","iopub.status.idle":"2023-08-05T22:18:12.469167Z","shell.execute_reply.started":"2023-08-05T22:18:12.463496Z","shell.execute_reply":"2023-08-05T22:18:12.467878Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def compare_models(df):\n    scores = []\n    for model, model_prediction in zip(models, predictions):\n        em_score = 0\n        f1_score = 0\n        for predicted_ans, actual_ans in zip(df[model_prediction], df['answer']):\n            em_score += compute_exact_match(predicted_ans, actual_ans)\n            f1_score += compute_f1(predicted_ans, actual_ans)\n        em_score /= len(df)\n        f1_score /= len(df)\n        scores.append({'Model': model, 'EM Score': em_score, 'F1 Score':f1_score})\n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.470853Z","iopub.execute_input":"2023-08-05T22:18:12.471570Z","iopub.status.idle":"2023-08-05T22:18:12.482379Z","shell.execute_reply.started":"2023-08-05T22:18:12.471526Z","shell.execute_reply":"2023-08-05T22:18:12.481176Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Computing overall score\ndev_scores = compare_models(val_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:12.485205Z","iopub.execute_input":"2023-08-05T22:18:12.485681Z","iopub.status.idle":"2023-08-05T22:18:22.271822Z","shell.execute_reply.started":"2023-08-05T22:18:12.485643Z","shell.execute_reply":"2023-08-05T22:18:22.270740Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"dev_score_df = pd.DataFrame(dev_scores)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:22.273473Z","iopub.execute_input":"2023-08-05T22:18:22.273878Z","iopub.status.idle":"2023-08-05T22:18:22.280252Z","shell.execute_reply.started":"2023-08-05T22:18:22.273839Z","shell.execute_reply":"2023-08-05T22:18:22.278667Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Evaluation Scores obtained for each models in the Dev Data","metadata":{}},{"cell_type":"code","source":"dev_score_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:22.282039Z","iopub.execute_input":"2023-08-05T22:18:22.282550Z","iopub.status.idle":"2023-08-05T22:18:22.302588Z","shell.execute_reply.started":"2023-08-05T22:18:22.282512Z","shell.execute_reply":"2023-08-05T22:18:22.301331Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.641278  0.731684\n1      Albert  0.616956  0.704614\n2     DeBerta  0.727089  0.797077\n3  DistilBert  0.621950  0.713455\n4     Electra  0.703378  0.796313","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.641278</td>\n      <td>0.731684</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.616956</td>\n      <td>0.704614</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.727089</td>\n      <td>0.797077</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.621950</td>\n      <td>0.713455</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.703378</td>\n      <td>0.796313</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Analysis for Has Answer and No Answer Questions","metadata":{}},{"cell_type":"code","source":"val_has_ans.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T00:42:28.413458Z","iopub.execute_input":"2023-08-06T00:42:28.413819Z","iopub.status.idle":"2023-08-06T00:42:28.441998Z","shell.execute_reply.started":"2023-08-06T00:42:28.413784Z","shell.execute_reply":"2023-08-06T00:42:28.440662Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                     qas_id  \\\n0  5737958b1c456719005744c4   \n1  572757bef1498d1400e8f694   \n2  5733fc6ed058e614000b6711   \n4  57308ddc396df919000961a4   \n5  5730088e947a6a140053cfb0   \n\n                                            question  \\\n0    [What, actually, causes, rigidity, in, matter?]   \n1  [School, desegregation, in, the, United, State...   \n2  [How, much, gun, powder, was, destroyed, in, a...   \n4  [Imperialism, is, most, often, associated, wit...   \n5  [What, long, term, agenda, was, the, acts, of,...   \n\n                                             context  \\\n0  [It, is, a, common, misconception, to, ascribe...   \n1  [In, many, parts, of, the, United, States,, af...   \n2  [Governor, Vaudreuil,, who, harboured, ambitio...   \n4  [The, principles, of, imperialism, are, often,...   \n5  [The, views, of, Ali, Shariati,, ideologue, of...   \n\n                          answer  is_impossible  start_pos  end_pos  \\\n0  the Pauli exclusion principle          False         33       36   \n1               African-American          False         65       65   \n2                  45,000 pounds          False         72       73   \n4             the British Empire          False         13       15   \n5                     conspiracy          False         94       94   \n\n                                   santiy_check           bert_predictions  \\\n0  [the, Pauli, exclusion, principle.[citation]  pauli exclusion principle   \n1                            [African-American]         african - american   \n2                              [45,000, pounds]            45 , 000 pounds   \n4                        [the, British, Empire]         the british empire   \n5                                  [conspiracy]                 conspiracy   \n\n                                  albert_predictions  \\\n0  [CLS] what actually causes rigidity in matter?...   \n1                                   african-american   \n2                                      45,000 pounds   \n4                                     british empire   \n5                                         conspiracy   \n\n         deberta_predictions  \\\n0  Pauli exclusion principle   \n1           African-American   \n2              45,000 pounds   \n4             British Empire   \n5                 conspiracy   \n\n                              distilbert_predictions  \\\n0                          pauli exclusion principle   \n1                                 african - american   \n2                                    45 , 000 pounds   \n4                                     british empire   \n5  [CLS] what long term agenda was the acts of pl...   \n\n         electra_predictions  \n0  pauli exclusion principle  \n1         african - american  \n2            45 , 000 pounds  \n4             british empire  \n5                 conspiracy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n      <th>bert_predictions</th>\n      <th>albert_predictions</th>\n      <th>deberta_predictions</th>\n      <th>distilbert_predictions</th>\n      <th>electra_predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5737958b1c456719005744c4</td>\n      <td>[What, actually, causes, rigidity, in, matter?]</td>\n      <td>[It, is, a, common, misconception, to, ascribe...</td>\n      <td>the Pauli exclusion principle</td>\n      <td>False</td>\n      <td>33</td>\n      <td>36</td>\n      <td>[the, Pauli, exclusion, principle.[citation]</td>\n      <td>pauli exclusion principle</td>\n      <td>[CLS] what actually causes rigidity in matter?...</td>\n      <td>Pauli exclusion principle</td>\n      <td>pauli exclusion principle</td>\n      <td>pauli exclusion principle</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>572757bef1498d1400e8f694</td>\n      <td>[School, desegregation, in, the, United, State...</td>\n      <td>[In, many, parts, of, the, United, States,, af...</td>\n      <td>African-American</td>\n      <td>False</td>\n      <td>65</td>\n      <td>65</td>\n      <td>[African-American]</td>\n      <td>african - american</td>\n      <td>african-american</td>\n      <td>African-American</td>\n      <td>african - american</td>\n      <td>african - american</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5733fc6ed058e614000b6711</td>\n      <td>[How, much, gun, powder, was, destroyed, in, a...</td>\n      <td>[Governor, Vaudreuil,, who, harboured, ambitio...</td>\n      <td>45,000 pounds</td>\n      <td>False</td>\n      <td>72</td>\n      <td>73</td>\n      <td>[45,000, pounds]</td>\n      <td>45 , 000 pounds</td>\n      <td>45,000 pounds</td>\n      <td>45,000 pounds</td>\n      <td>45 , 000 pounds</td>\n      <td>45 , 000 pounds</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57308ddc396df919000961a4</td>\n      <td>[Imperialism, is, most, often, associated, wit...</td>\n      <td>[The, principles, of, imperialism, are, often,...</td>\n      <td>the British Empire</td>\n      <td>False</td>\n      <td>13</td>\n      <td>15</td>\n      <td>[the, British, Empire]</td>\n      <td>the british empire</td>\n      <td>british empire</td>\n      <td>British Empire</td>\n      <td>british empire</td>\n      <td>british empire</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5730088e947a6a140053cfb0</td>\n      <td>[What, long, term, agenda, was, the, acts, of,...</td>\n      <td>[The, views, of, Ali, Shariati,, ideologue, of...</td>\n      <td>conspiracy</td>\n      <td>False</td>\n      <td>94</td>\n      <td>94</td>\n      <td>[conspiracy]</td>\n      <td>conspiracy</td>\n      <td>conspiracy</td>\n      <td>conspiracy</td>\n      <td>[CLS] what long term agenda was the acts of pl...</td>\n      <td>conspiracy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_no_ans.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T00:42:34.494418Z","iopub.execute_input":"2023-08-06T00:42:34.494797Z","iopub.status.idle":"2023-08-06T00:42:34.526882Z","shell.execute_reply.started":"2023-08-06T00:42:34.494767Z","shell.execute_reply":"2023-08-06T00:42:34.525773Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"                      qas_id  \\\n3   5a67b94bf038b7001ab0c444   \n14  5ad266f6d7d075001a429200   \n16  5a2c41c8bfd06b001a5aeaa8   \n17  5ad3f6f5604f3c001a3ffa09   \n21  5ad40ac5604f3c001a3fffc8   \n\n                                             question  \\\n3   [How, many, Swedish, students, were, enrolled,...   \n14  [If, an, internal, force, acts, on, the, syste...   \n16  [When, did, Microsoft, decide, to, appeal, the...   \n17  [Where, did, the, Normans, invade, in, the, 11...   \n21   [What, type, of, medicine, did, otachi, reject?]   \n\n                                              context answer  is_impossible  \\\n3   [In, Sweden,, pupils, are, free, to, choose, a...                  True   \n14  [This, means, that, in, a, closed, system, of,...                  True   \n16  [In, July, 2013,, the, English, High, Court, o...                  True   \n17  [The, Normans, had, a, profound, effect, on, I...                  True   \n21  [The, physicians, of, the, Yuan, court, came, ...                  True   \n\n    start_pos  end_pos santiy_check bert_predictions albert_predictions  \\\n3          -1       -1           []                                       \n14         -1       -1           []                                       \n16         -1       -1           []                                       \n17         -1       -1           []                                       \n21         -1       -1           []                                       \n\n   deberta_predictions distilbert_predictions electra_predictions  \n3                                                                  \n14                                                                 \n16                                                                 \n17                                                                 \n21                                                                 ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n      <th>bert_predictions</th>\n      <th>albert_predictions</th>\n      <th>deberta_predictions</th>\n      <th>distilbert_predictions</th>\n      <th>electra_predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>5a67b94bf038b7001ab0c444</td>\n      <td>[How, many, Swedish, students, were, enrolled,...</td>\n      <td>[In, Sweden,, pupils, are, free, to, choose, a...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5ad266f6d7d075001a429200</td>\n      <td>[If, an, internal, force, acts, on, the, syste...</td>\n      <td>[This, means, that, in, a, closed, system, of,...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5a2c41c8bfd06b001a5aeaa8</td>\n      <td>[When, did, Microsoft, decide, to, appeal, the...</td>\n      <td>[In, July, 2013,, the, English, High, Court, o...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5ad3f6f5604f3c001a3ffa09</td>\n      <td>[Where, did, the, Normans, invade, in, the, 11...</td>\n      <td>[The, Normans, had, a, profound, effect, on, I...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>5ad40ac5604f3c001a3fffc8</td>\n      <td>[What, type, of, medicine, did, otachi, reject?]</td>\n      <td>[The, physicians, of, the, Yuan, court, came, ...</td>\n      <td></td>\n      <td>True</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>[]</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"has_ans_scores = compare_models(val_has_ans)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:22.406887Z","iopub.execute_input":"2023-08-05T22:18:22.407337Z","iopub.status.idle":"2023-08-05T22:18:31.461360Z","shell.execute_reply.started":"2023-08-05T22:18:22.407296Z","shell.execute_reply":"2023-08-05T22:18:31.460182Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"has_ans_score_df = pd.DataFrame(has_ans_scores)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:31.462827Z","iopub.execute_input":"2023-08-05T22:18:31.463315Z","iopub.status.idle":"2023-08-05T22:18:31.470151Z","shell.execute_reply.started":"2023-08-05T22:18:31.463253Z","shell.execute_reply":"2023-08-05T22:18:31.468877Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"Evaluation Scores for Samples that have golden truth answer","metadata":{}},{"cell_type":"code","source":"has_ans_score_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:31.472105Z","iopub.execute_input":"2023-08-05T22:18:31.472527Z","iopub.status.idle":"2023-08-05T22:18:31.493037Z","shell.execute_reply.started":"2023-08-05T22:18:31.472488Z","shell.execute_reply":"2023-08-05T22:18:31.491968Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.536156  0.653055\n1      Albert  0.504707  0.618053\n2     DeBerta  0.647114  0.737612\n3  DistilBert  0.511165  0.629484\n4     Electra  0.616454  0.736624","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.536156</td>\n      <td>0.653055</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.504707</td>\n      <td>0.618053</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.647114</td>\n      <td>0.737612</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.511165</td>\n      <td>0.629484</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.616454</td>\n      <td>0.736624</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"no_ans_scores = compare_models(val_no_ans)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:31.494890Z","iopub.execute_input":"2023-08-05T22:18:31.495287Z","iopub.status.idle":"2023-08-05T22:18:32.480462Z","shell.execute_reply.started":"2023-08-05T22:18:31.495227Z","shell.execute_reply":"2023-08-05T22:18:32.479310Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"no_ans_score_df = pd.DataFrame(no_ans_scores)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:32.482141Z","iopub.execute_input":"2023-08-05T22:18:32.482547Z","iopub.status.idle":"2023-08-05T22:18:32.488802Z","shell.execute_reply.started":"2023-08-05T22:18:32.482508Z","shell.execute_reply":"2023-08-05T22:18:32.487587Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Evaluation Scores for Samples that have no golden truth answer","metadata":{}},{"cell_type":"code","source":"no_ans_score_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:32.490778Z","iopub.execute_input":"2023-08-05T22:18:32.491202Z","iopub.status.idle":"2023-08-05T22:18:32.510064Z","shell.execute_reply.started":"2023-08-05T22:18:32.491163Z","shell.execute_reply":"2023-08-05T22:18:32.508636Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert       1.0       1.0\n1      Albert       1.0       1.0\n2     DeBerta       1.0       1.0\n3  DistilBert       1.0       1.0\n4     Electra       1.0       1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Question Type Analysis","metadata":{}},{"cell_type":"code","source":"question_types = ['What', 'Who', 'When', 'Where', 'Why', 'How', 'Other']\nqn_type_scores = []\nval_what_df = val_df[val_df['question_type']=='What']\nval_who_df = val_df[val_df['question_type']=='Who']\nval_when_df = val_df[val_df['question_type']=='When']\nval_where_df = val_df[val_df['question_type']=='Where']\nval_why_df = val_df[val_df['question_type']=='Why']\nval_how_df = val_df[val_df['question_type']=='How']\nval_other_df = val_df[val_df['question_type']=='Other']","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:32.512067Z","iopub.execute_input":"2023-08-05T22:18:32.512501Z","iopub.status.idle":"2023-08-05T22:18:32.587309Z","shell.execute_reply.started":"2023-08-05T22:18:32.512464Z","shell.execute_reply":"2023-08-05T22:18:32.586230Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### What Type","metadata":{}},{"cell_type":"code","source":"what_scores = compare_models(val_what_df)\nwhat_scores_df = pd.DataFrame(what_scores)\nwhat_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:32.589022Z","iopub.execute_input":"2023-08-05T22:18:32.589439Z","iopub.status.idle":"2023-08-05T22:18:38.557841Z","shell.execute_reply.started":"2023-08-05T22:18:32.589402Z","shell.execute_reply":"2023-08-05T22:18:38.556548Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.634664  0.722074\n1      Albert  0.605488  0.689604\n2     DeBerta  0.717242  0.780953\n3  DistilBert  0.615449  0.702584\n4     Electra  0.704068  0.790446","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.634664</td>\n      <td>0.722074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.605488</td>\n      <td>0.689604</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.717242</td>\n      <td>0.780953</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.615449</td>\n      <td>0.702584</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.704068</td>\n      <td>0.790446</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Who Type","metadata":{}},{"cell_type":"code","source":"who_scores = compare_models(val_who_df)\nwho_scores_df = pd.DataFrame(who_scores)\nwho_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:38.559671Z","iopub.execute_input":"2023-08-05T22:18:38.560132Z","iopub.status.idle":"2023-08-05T22:18:39.307557Z","shell.execute_reply.started":"2023-08-05T22:18:38.560092Z","shell.execute_reply":"2023-08-05T22:18:39.306237Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.739531  0.812106\n1      Albert  0.706857  0.781374\n2     DeBerta  0.793373  0.845801\n3  DistilBert  0.739991  0.813007\n4     Electra  0.795214  0.865493","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.739531</td>\n      <td>0.812106</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.706857</td>\n      <td>0.781374</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.793373</td>\n      <td>0.845801</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.739991</td>\n      <td>0.813007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.795214</td>\n      <td>0.865493</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### When Type","metadata":{}},{"cell_type":"code","source":"when_scores = compare_models(val_when_df)\nwhen_scores_df = pd.DataFrame(when_scores)\nwhen_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:39.309116Z","iopub.execute_input":"2023-08-05T22:18:39.310142Z","iopub.status.idle":"2023-08-05T22:18:39.932620Z","shell.execute_reply.started":"2023-08-05T22:18:39.310095Z","shell.execute_reply":"2023-08-05T22:18:39.931272Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.714617  0.806692\n1      Albert  0.693735  0.777122\n2     DeBerta  0.757541  0.830342\n3  DistilBert  0.717517  0.806435\n4     Electra  0.756381  0.852898","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.714617</td>\n      <td>0.806692</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.693735</td>\n      <td>0.777122</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.757541</td>\n      <td>0.830342</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.717517</td>\n      <td>0.806435</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.756381</td>\n      <td>0.852898</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Where Type","metadata":{}},{"cell_type":"code","source":"where_scores = compare_models(val_where_df)\nwhere_scores_df = pd.DataFrame(where_scores)\nwhere_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:39.934556Z","iopub.execute_input":"2023-08-05T22:18:39.935042Z","iopub.status.idle":"2023-08-05T22:18:40.417551Z","shell.execute_reply.started":"2023-08-05T22:18:39.934996Z","shell.execute_reply":"2023-08-05T22:18:40.416295Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.617481  0.736341\n1      Albert  0.556391  0.687157\n2     DeBerta  0.667293  0.778904\n3  DistilBert  0.546053  0.671162\n4     Electra  0.661654  0.794295","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.617481</td>\n      <td>0.736341</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.556391</td>\n      <td>0.687157</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.667293</td>\n      <td>0.778904</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.546053</td>\n      <td>0.671162</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.661654</td>\n      <td>0.794295</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Why Type","metadata":{}},{"cell_type":"code","source":"why_scores = compare_models(val_why_df)\nwhy_scores_df = pd.DataFrame(why_scores)\nwhy_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:40.419095Z","iopub.execute_input":"2023-08-05T22:18:40.422653Z","iopub.status.idle":"2023-08-05T22:18:40.740431Z","shell.execute_reply.started":"2023-08-05T22:18:40.422529Z","shell.execute_reply":"2023-08-05T22:18:40.736250Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.473214  0.663275\n1      Albert  0.415179  0.625578\n2     DeBerta  0.495536  0.713588\n3  DistilBert  0.404018  0.647365\n4     Electra  0.497768  0.725139","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.473214</td>\n      <td>0.663275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.415179</td>\n      <td>0.625578</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.495536</td>\n      <td>0.713588</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.404018</td>\n      <td>0.647365</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.497768</td>\n      <td>0.725139</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### How Type","metadata":{}},{"cell_type":"code","source":"how_scores = compare_models(val_how_df)\nhow_scores_df = pd.DataFrame(how_scores)\nhow_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:40.746576Z","iopub.execute_input":"2023-08-05T22:18:40.747103Z","iopub.status.idle":"2023-08-05T22:18:41.960459Z","shell.execute_reply.started":"2023-08-05T22:18:40.747052Z","shell.execute_reply":"2023-08-05T22:18:41.959318Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.587697  0.690993\n1      Albert  0.592457  0.695071\n2     DeBerta  0.757232  0.846101\n3  DistilBert  0.569755  0.675160\n4     Electra  0.628707  0.749112","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.587697</td>\n      <td>0.690993</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.592457</td>\n      <td>0.695071</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.757232</td>\n      <td>0.846101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.569755</td>\n      <td>0.675160</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.628707</td>\n      <td>0.749112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Other Type","metadata":{}},{"cell_type":"code","source":"other_scores = compare_models(val_other_df)\nother_scores_df = pd.DataFrame(other_scores)\nother_scores_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:41.963521Z","iopub.execute_input":"2023-08-05T22:18:41.964462Z","iopub.status.idle":"2023-08-05T22:18:43.331567Z","shell.execute_reply.started":"2023-08-05T22:18:41.964415Z","shell.execute_reply":"2023-08-05T22:18:43.330424Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"        Model  EM Score  F1 Score\n0        Bert  0.645200  0.724686\n1      Albert  0.645595  0.713230\n2     DeBerta  0.743580  0.801240\n3  DistilBert  0.622284  0.702282\n4     Electra  0.718688  0.798826","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>EM Score</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>0.645200</td>\n      <td>0.724686</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>0.645595</td>\n      <td>0.713230</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>0.743580</td>\n      <td>0.801240</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>0.622284</td>\n      <td>0.702282</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>0.718688</td>\n      <td>0.798826</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## False No Answer Predictions","metadata":{}},{"cell_type":"code","source":"false_empty_counts = []\nfor model, model_prediction in zip(models, predictions):\n    count = val_df[((val_df[model_prediction] == '')) & (val_df['answer'] != '')].shape[0]\n    ratio = count/len(val_df)\n    false_empty_counts.append({'Model': model, 'count': count, 'incorrect_ratio': ratio})","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.337013Z","iopub.execute_input":"2023-08-05T22:18:43.339947Z","iopub.status.idle":"2023-08-05T22:18:43.442004Z","shell.execute_reply.started":"2023-08-05T22:18:43.339899Z","shell.execute_reply":"2023-08-05T22:18:43.440786Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"false_empty_preds_df = pd.DataFrame(false_empty_counts)\nfalse_empty_preds_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.447992Z","iopub.execute_input":"2023-08-05T22:18:43.451257Z","iopub.status.idle":"2023-08-05T22:18:43.472488Z","shell.execute_reply.started":"2023-08-05T22:18:43.451195Z","shell.execute_reply":"2023-08-05T22:18:43.471359Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"        Model  count  incorrect_ratio\n0        Bert   1127         0.042963\n1      Albert   1959         0.074680\n2     DeBerta   1592         0.060689\n3  DistilBert   1486         0.056648\n4     Electra   1578         0.060156","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>count</th>\n      <th>incorrect_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>1127</td>\n      <td>0.042963</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>1959</td>\n      <td>0.074680</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>1592</td>\n      <td>0.060689</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>1486</td>\n      <td>0.056648</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>1578</td>\n      <td>0.060156</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Qn Length Analysis","metadata":{}},{"cell_type":"code","source":"val_df[\"Answer Length\"] = val_df[\"answer\"].apply(len)\nval_df[\"Answer Length\"].describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.498002Z","iopub.execute_input":"2023-08-05T22:18:43.502281Z","iopub.status.idle":"2023-08-05T22:18:43.551515Z","shell.execute_reply.started":"2023-08-05T22:18:43.502226Z","shell.execute_reply":"2023-08-05T22:18:43.550354Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        15.461383\nstd         19.323377\nmin          0.000000\n25%          3.000000\n50%         10.000000\n75%         20.000000\nmax        160.000000\nName: Answer Length, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"val_df[\"bert_predictions\"].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.556917Z","iopub.execute_input":"2023-08-05T22:18:43.559999Z","iopub.status.idle":"2023-08-05T22:18:43.607232Z","shell.execute_reply.started":"2023-08-05T22:18:43.559932Z","shell.execute_reply":"2023-08-05T22:18:43.606167Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        86.125877\nstd        204.713788\nmin          0.000000\n25%          0.000000\n50%         12.000000\n75%         37.250000\nmax       2612.000000\nName: bert_predictions, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"val_df[\"albert_predictions\"].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.613008Z","iopub.execute_input":"2023-08-05T22:18:43.616046Z","iopub.status.idle":"2023-08-05T22:18:43.664838Z","shell.execute_reply.started":"2023-08-05T22:18:43.615991Z","shell.execute_reply":"2023-08-05T22:18:43.663701Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        99.688548\nstd        214.965429\nmin          0.000000\n25%          0.000000\n50%         12.000000\n75%         51.000000\nmax       2160.000000\nName: albert_predictions, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"val_df[\"deberta_predictions\"].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.670508Z","iopub.execute_input":"2023-08-05T22:18:43.673498Z","iopub.status.idle":"2023-08-05T22:18:43.725950Z","shell.execute_reply.started":"2023-08-05T22:18:43.673442Z","shell.execute_reply":"2023-08-05T22:18:43.724082Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        67.871150\nstd        174.099937\nmin          0.000000\n25%          0.000000\n50%         11.000000\n75%         28.000000\nmax       1832.000000\nName: deberta_predictions, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"val_df[\"distilbert_predictions\"].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.731386Z","iopub.execute_input":"2023-08-05T22:18:43.734200Z","iopub.status.idle":"2023-08-05T22:18:43.780974Z","shell.execute_reply.started":"2023-08-05T22:18:43.734152Z","shell.execute_reply":"2023-08-05T22:18:43.779826Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        90.005985\nstd        211.544306\nmin          0.000000\n25%          0.000000\n50%         12.000000\n75%         39.000000\nmax       2612.000000\nName: distilbert_predictions, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"val_df[\"electra_predictions\"].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.786924Z","iopub.execute_input":"2023-08-05T22:18:43.789088Z","iopub.status.idle":"2023-08-05T22:18:43.824044Z","shell.execute_reply.started":"2023-08-05T22:18:43.789028Z","shell.execute_reply":"2023-08-05T22:18:43.822451Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"count    26232.000000\nmean        40.486276\nstd        118.138139\nmin          0.000000\n25%          0.000000\n50%         11.000000\n75%         24.000000\nmax       1739.000000\nName: electra_predictions, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"answer_length_comp = []\nfor model, model_prediction in zip(models, predictions):\n    val_df[\"Answer Length Difference\"] = val_df[model_prediction].apply(lambda x: len(x.split())) - val_df[\"answer\"].apply(lambda x: len(x.split()))\n\n    # Identify if answers are too short, too long, or within a certain range\n    short_threshold = -5  # Define the threshold for too short\n    long_threshold = 5    # Define the threshold for too long\n\n    val_df[\"Answer Length Category\"] = val_df[\"Answer Length Difference\"].apply(\n        lambda diff: \"Too Short\" if diff < short_threshold else (\n            \"Too Long\" if diff > long_threshold else \"Within Range\"\n        )\n    )\n\n    # Count the occurrences of each category\n    category_counts = val_df[\"Answer Length Category\"].value_counts()\n    ans_len_result = {'Model': model}\n    ans_len_result.update(category_counts.to_dict())\n    answer_length_comp.append(ans_len_result)\n\nans_len_df = pd.DataFrame(answer_length_comp)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:43.825921Z","iopub.execute_input":"2023-08-05T22:18:43.826371Z","iopub.status.idle":"2023-08-05T22:18:44.294389Z","shell.execute_reply.started":"2023-08-05T22:18:43.826327Z","shell.execute_reply":"2023-08-05T22:18:44.293283Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"ans_len_df","metadata":{"execution":{"iopub.status.busy":"2023-08-05T22:18:44.296206Z","iopub.execute_input":"2023-08-05T22:18:44.296654Z","iopub.status.idle":"2023-08-05T22:18:44.310108Z","shell.execute_reply.started":"2023-08-05T22:18:44.296616Z","shell.execute_reply":"2023-08-05T22:18:44.308484Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"        Model  Within Range  Too Long  Too Short\n0        Bert         20975      5077        180\n1      Albert         20104      5876        252\n2     DeBerta         22301      3677        254\n3  DistilBert         20731      5288        213\n4     Electra         23489      2451        292","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Within Range</th>\n      <th>Too Long</th>\n      <th>Too Short</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bert</td>\n      <td>20975</td>\n      <td>5077</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albert</td>\n      <td>20104</td>\n      <td>5876</td>\n      <td>252</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DeBerta</td>\n      <td>22301</td>\n      <td>3677</td>\n      <td>254</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DistilBert</td>\n      <td>20731</td>\n      <td>5288</td>\n      <td>213</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Electra</td>\n      <td>23489</td>\n      <td>2451</td>\n      <td>292</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}