{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport json\nimport transformers\nimport torch\nimport random\nfrom torch import cuda\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import ElectraTokenizer, ElectraModel, ElectraForQuestionAnswering","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-04T11:10:29.284465Z","iopub.execute_input":"2023-08-04T11:10:29.284826Z","iopub.status.idle":"2023-08-04T11:10:43.134451Z","shell.execute_reply.started":"2023-08-04T11:10:29.284794Z","shell.execute_reply":"2023-08-04T11:10:43.133490Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setting the random seed for consistent results\nrandom_seed = 42\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(random_seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:43.136329Z","iopub.execute_input":"2023-08-04T11:10:43.136664Z","iopub.status.idle":"2023-08-04T11:10:43.173814Z","shell.execute_reply.started":"2023-08-04T11:10:43.136631Z","shell.execute_reply":"2023-08-04T11:10:43.172854Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:43.176055Z","iopub.execute_input":"2023-08-04T11:10:43.176699Z","iopub.status.idle":"2023-08-04T11:10:43.181180Z","shell.execute_reply.started":"2023-08-04T11:10:43.176664Z","shell.execute_reply":"2023-08-04T11:10:43.180237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:43.183812Z","iopub.execute_input":"2023-08-04T11:10:43.184474Z","iopub.status.idle":"2023-08-04T11:10:43.196573Z","shell.execute_reply.started":"2023-08-04T11:10:43.184436Z","shell.execute_reply":"2023-08-04T11:10:43.195475Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading the data","metadata":{}},{"cell_type":"code","source":"# Loading train data\ntrain_file = open('/kaggle/input/squad-2/train-v2.0.json')\ntrain_data = json.load(train_file)\n\n# Loading validation data\nval_file = open('/kaggle/input/squad-2/dev-v2.0.json')\nval_data = json.load(val_file)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:43.198009Z","iopub.execute_input":"2023-08-04T11:10:43.198604Z","iopub.status.idle":"2023-08-04T11:10:45.008820Z","shell.execute_reply.started":"2023-08-04T11:10:43.198571Z","shell.execute_reply":"2023-08-04T11:10:45.007708Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing the dataset","metadata":{}},{"cell_type":"code","source":"tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:45.010468Z","iopub.execute_input":"2023-08-04T11:10:45.011137Z","iopub.status.idle":"2023-08-04T11:10:45.948885Z","shell.execute_reply.started":"2023-08-04T11:10:45.011102Z","shell.execute_reply":"2023-08-04T11:10:45.947941Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b90e95d1294db4b1f184960526726c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccff12d72364ae9a64eec60d864878e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268a3cd3e9304aada91e0815c7f3c485"}},"metadata":{}}]},{"cell_type":"code","source":"def index_converter(context, context_tokenized):\n    \"\"\"\n    Maps start str index to a tokenized index\n    \"\"\"\n    mapper = {}\n    curr = ''\n    token_idx = 0\n    for i, char in enumerate(context):\n        if char != ' ' and char != '\\n' and char != '\\t' and char != '\\r': # making sure current char is not whitespace\n            curr += char\n            if curr == context_tokenized[token_idx]:\n                start = i - len(curr) + 1\n                for j in range(start, i+1):\n                    mapper[j] = (curr, token_idx)                \n                curr = ''\n                token_idx += 1\n    if token_idx != len(context_tokenized): # skipping the data in case of issue with spanning\n        return None\n    return mapper\n\ndef preprocess_data(dataset, is_training=True, tokenized=True):\n    \"\"\"\n    Parse the json_data object into a pandas readable data representation (list of dicts)\n    \"\"\"\n    \n    def _tokenize(seq):\n        \"\"\"\n        Minimizes errors between tokenizers and encodings.\n        Recommended in the paper BiDAF (Seo et al., 2016)\n        \"\"\"\n        return [t.replace(\"``\", '\"').replace(\"''\", '\"') for t in seq.split()]\n    \n    examples = [] # store rows of data here for qa\n    \n    tokenization_errors = 0\n    misaligned_ans_errors = 0\n    num_impossibles = 0\n    num_questions = 0\n    \n    for article_id in tqdm(range(len(dataset['data']))): # for each context\n        paragraphs = dataset['data'][article_id]['paragraphs']\n        for paragraph_id in range(len(paragraphs)):\n            questions = dataset['data'][article_id]['paragraphs'][paragraph_id]['qas']\n            \n            context = paragraphs[paragraph_id]['context']\n            context_tokenized = _tokenize(context)\n                    \n            for qid in range(len(questions)): # loop through questions\n                num_questions += 1\n                \n                question = questions[qid]['question']\n                question_tokenized = _tokenize(question)\n                qas_id = questions[qid]['id']\n                \n                is_impossible = questions[qid]['is_impossible']\n                \n                if is_impossible: # check if question is impossible to answer\n                    num_impossibles += 1\n                    examples.append({'qas_id': qas_id, \n                                     'question':question_tokenized if tokenized else question, \n                                     'context': context_tokenized if tokenized else context, \n                                     'answer':'', \n                                     'is_impossible': is_impossible,\n                                     'start_pos': -1, \n                                     'end_pos':-1,\n                                    'santiy_check': context_tokenized[-1:0]})\n                    continue\n                    \n                # question is not impossible, continue parsing\n                answers = questions[qid]['answers']\n                \n                for ans_id in range(len(answers)): # for each answer\n                    answer = answers[ans_id]['text']\n                    start_pos = answers[ans_id]['answer_start'] # inclusive start index in raw context\n                    end_pos = start_pos + len(answer) #exclusive end index in raw context\n                          \n                    if context[start_pos:end_pos] != answer:\n                        misaligned_ans_errors += 1\n                        continue\n                        \n                    if tokenized:\n                        mapper = index_converter(context, context_tokenized)\n                        if mapper is None:\n                            tokenization_errors += 1\n                            continue\n                        \n                        start_pos = mapper[start_pos][1]\n                        end_pos = mapper[end_pos-1][1] # inclusive\n                    \n                    examples.append({'qas_id': qas_id, \n                                     'question':question_tokenized if tokenized else question, \n                                     'context': context_tokenized if tokenized else context, \n                                     'answer':answer, \n                                     'is_impossible': is_impossible,\n                                     'start_pos': start_pos, \n                                     'end_pos':end_pos,\n                                    'santiy_check': context_tokenized[start_pos:end_pos+1] if tokenized else context[start_pos:end_pos+1]})\n            \n                    \n    print('No. of questions:{}'.format(num_questions))\n    return examples","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:45.950511Z","iopub.execute_input":"2023-08-04T11:10:45.951073Z","iopub.status.idle":"2023-08-04T11:10:45.971071Z","shell.execute_reply.started":"2023-08-04T11:10:45.951037Z","shell.execute_reply":"2023-08-04T11:10:45.969922Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing of Train data","metadata":{}},{"cell_type":"code","source":"train_processed = preprocess_data(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:10:45.972643Z","iopub.execute_input":"2023-08-04T11:10:45.973018Z","iopub.status.idle":"2023-08-04T11:11:26.411716Z","shell.execute_reply.started":"2023-08-04T11:10:45.972984Z","shell.execute_reply":"2023-08-04T11:11:26.410637Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 442/442 [00:40<00:00, 10.94it/s]","output_type":"stream"},{"name":"stdout","text":"No. of questions:130319\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.DataFrame(train_processed)\ntrain_df[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:11:26.413198Z","iopub.execute_input":"2023-08-04T11:11:26.413644Z","iopub.status.idle":"2023-08-04T11:11:26.817083Z","shell.execute_reply.started":"2023-08-04T11:11:26.413599Z","shell.execute_reply":"2023-08-04T11:11:26.816085Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                     qas_id  \\\n0  56be85543aeaaa14008c9063   \n1  56be85543aeaaa14008c9065   \n2  56be85543aeaaa14008c9066   \n3  56bf6b0f3aeaaa14008c9601   \n4  56bf6b0f3aeaaa14008c9602   \n\n                                            question  \\\n0    [When, did, Beyonce, start, becoming, popular?]   \n1  [What, areas, did, Beyonce, compete, in, when,...   \n2  [When, did, Beyonce, leave, Destiny's, Child, ...   \n3  [In, what, city, and, state, did, Beyonce, gro...   \n4  [In, which, decade, did, Beyonce, become, famo...   \n\n                                             context               answer  \\\n0  [Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...    in the late 1990s   \n1  [Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...  singing and dancing   \n2  [Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...                 2003   \n3  [Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...       Houston, Texas   \n4  [Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...           late 1990s   \n\n   is_impossible  start_pos  end_pos             santiy_check  \n0          False         39       42   [in, the, late, 1990s]  \n1          False         28       30  [singing, and, dancing]  \n2          False         82       82                [(2003),]  \n3          False         22       23       [Houston,, Texas,]  \n4          False         41       42            [late, 1990s]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56be85543aeaaa14008c9063</td>\n      <td>[When, did, Beyonce, start, becoming, popular?]</td>\n      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n      <td>in the late 1990s</td>\n      <td>False</td>\n      <td>39</td>\n      <td>42</td>\n      <td>[in, the, late, 1990s]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56be85543aeaaa14008c9065</td>\n      <td>[What, areas, did, Beyonce, compete, in, when,...</td>\n      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n      <td>singing and dancing</td>\n      <td>False</td>\n      <td>28</td>\n      <td>30</td>\n      <td>[singing, and, dancing]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56be85543aeaaa14008c9066</td>\n      <td>[When, did, Beyonce, leave, Destiny's, Child, ...</td>\n      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n      <td>2003</td>\n      <td>False</td>\n      <td>82</td>\n      <td>82</td>\n      <td>[(2003),]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56bf6b0f3aeaaa14008c9601</td>\n      <td>[In, what, city, and, state, did, Beyonce, gro...</td>\n      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n      <td>Houston, Texas</td>\n      <td>False</td>\n      <td>22</td>\n      <td>23</td>\n      <td>[Houston,, Texas,]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>56bf6b0f3aeaaa14008c9602</td>\n      <td>[In, which, decade, did, Beyonce, become, famo...</td>\n      <td>[Beyoncé, Giselle, Knowles-Carter, (/biːˈjɒnse...</td>\n      <td>late 1990s</td>\n      <td>False</td>\n      <td>41</td>\n      <td>42</td>\n      <td>[late, 1990s]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def encode_data(processed_data, tokenizer, max_len, max_query_len):\n    \"\"\"\n    Converts examples of data into Electra input format tensors.\n    \"\"\"\n    context_length_errors = 0\n    encoded_data = []\n    for sample in tqdm(processed_data):\n        question_raw = ' '.join(sample['question'])\n        context_raw = ' '.join(sample['context'])\n        if len(question_raw) > max_query_len:\n            question_raw = question_raw[:max_query_len]\n        \n        # encode the data using the tokenizer\n        encoded = tokenizer.encode_plus(question_raw, context_raw,\n                                        max_length=max_len,\n                                        padding='max_length',\n                                        truncation='only_second',\n                                        return_token_type_ids=True)\n        if sample['is_impossible']:\n            start = -1\n            end = -1\n        else: # Adjust the start_pos and end_pos \n            input_ids = encoded['input_ids']\n            answer_ids = tokenizer.encode(sample['answer']) \n            start, end = 0, 0\n            for i in range(len(input_ids)):\n                if input_ids[i: i+len(answer_ids[1:-1])] == answer_ids[1:-1]:\n                    start = i\n                    end = i + len(answer_ids[1:-1]) - 1\n                    break\n            \n        ids = encoded['input_ids']\n        token_type_ids = encoded['token_type_ids']\n        mask = encoded['attention_mask']\n        \n        assert len(ids) == max_len\n        assert len(token_type_ids) == max_len\n        assert len(mask) == max_len\n        \n        encoded_data.append({'ids': ids,\n                      'token_type_ids': token_type_ids,\n                      'mask': mask,\n                      'start_pos': start,\n                      'end_pos': end})        \n    return encoded_data","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:11:26.821784Z","iopub.execute_input":"2023-08-04T11:11:26.822114Z","iopub.status.idle":"2023-08-04T11:11:26.834522Z","shell.execute_reply.started":"2023-08-04T11:11:26.822081Z","shell.execute_reply":"2023-08-04T11:11:26.833562Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Model Settings\nMAX_SEQ_LEN = 512\nMAX_QN_LEN = 128\nNO_EPOCHS = 3\nBATCH_SIZE = 8\nLEARNING_RATE = 5e-05\nOUT = 2","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:11:26.835731Z","iopub.execute_input":"2023-08-04T11:11:26.836592Z","iopub.status.idle":"2023-08-04T11:11:26.851010Z","shell.execute_reply.started":"2023-08-04T11:11:26.836556Z","shell.execute_reply":"2023-08-04T11:11:26.850093Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_encoded = encode_data(train_processed, tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:11:26.852594Z","iopub.execute_input":"2023-08-04T11:11:26.853343Z","iopub.status.idle":"2023-08-04T11:23:59.448281Z","shell.execute_reply.started":"2023-08-04T11:11:26.853309Z","shell.execute_reply":"2023-08-04T11:23:59.447149Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 130217/130217 [12:32<00:00, 173.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creation of dataloader\ninput_ids = torch.tensor([sample['ids'] for sample in train_encoded], dtype=torch.long)\ninput_masks = torch.tensor([sample['mask'] for sample in train_encoded], dtype=torch.long)\nsegment_ids = torch.tensor([sample['token_type_ids'] for sample in train_encoded], dtype=torch.long)\n\nstart_positions = torch.tensor([sample['start_pos'] for sample in train_encoded], dtype=torch.long)\nend_positions = torch.tensor([sample['end_pos'] for sample in train_encoded], dtype=torch.long)\ntrain_dataset = TensorDataset(input_ids, input_masks, segment_ids, start_positions, end_positions)\n\ntrain_params = {'batch_size': BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(train_dataset, **train_params)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:23:59.449750Z","iopub.execute_input":"2023-08-04T11:23:59.450724Z","iopub.status.idle":"2023-08-04T11:24:28.084365Z","shell.execute_reply.started":"2023-08-04T11:23:59.450687Z","shell.execute_reply":"2023-08-04T11:24:28.082377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"val_processed = preprocess_data(val_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:24:28.087470Z","iopub.execute_input":"2023-08-04T11:24:28.088821Z","iopub.status.idle":"2023-08-04T11:24:38.153483Z","shell.execute_reply.started":"2023-08-04T11:24:28.088734Z","shell.execute_reply":"2023-08-04T11:24:38.152551Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 35/35 [00:10<00:00,  3.48it/s]","output_type":"stream"},{"name":"stdout","text":"No. of questions:11873\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Preprocessing of Validation Data","metadata":{}},{"cell_type":"code","source":"val_df = pd.DataFrame(val_processed)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:24:38.154960Z","iopub.execute_input":"2023-08-04T11:24:38.155571Z","iopub.status.idle":"2023-08-04T11:24:38.228553Z","shell.execute_reply.started":"2023-08-04T11:24:38.155535Z","shell.execute_reply":"2023-08-04T11:24:38.227554Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"val_df[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:24:38.230036Z","iopub.execute_input":"2023-08-04T11:24:38.230423Z","iopub.status.idle":"2023-08-04T11:24:38.254401Z","shell.execute_reply.started":"2023-08-04T11:24:38.230384Z","shell.execute_reply":"2023-08-04T11:24:38.253411Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                     qas_id                                     question  \\\n0  56ddde6b9a695914005b9628  [In, what, country, is, Normandy, located?]   \n1  56ddde6b9a695914005b9628  [In, what, country, is, Normandy, located?]   \n2  56ddde6b9a695914005b9628  [In, what, country, is, Normandy, located?]   \n3  56ddde6b9a695914005b9628  [In, what, country, is, Normandy, located?]   \n4  56ddde6b9a695914005b9629    [When, were, the, Normans, in, Normandy?]   \n\n                                             context                   answer  \\\n0  [The, Normans, (Norman:, Nourmands;, French:, ...                   France   \n1  [The, Normans, (Norman:, Nourmands;, French:, ...                   France   \n2  [The, Normans, (Norman:, Nourmands;, French:, ...                   France   \n3  [The, Normans, (Norman:, Nourmands;, French:, ...                   France   \n4  [The, Normans, (Norman:, Nourmands;, French:, ...  10th and 11th centuries   \n\n   is_impossible  start_pos  end_pos                  santiy_check  \n0          False         26       26                     [France.]  \n1          False         26       26                     [France.]  \n2          False         26       26                     [France.]  \n3          False         26       26                     [France.]  \n4          False         14       17  [10th, and, 11th, centuries]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qas_id</th>\n      <th>question</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>is_impossible</th>\n      <th>start_pos</th>\n      <th>end_pos</th>\n      <th>santiy_check</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56ddde6b9a695914005b9628</td>\n      <td>[In, what, country, is, Normandy, located?]</td>\n      <td>[The, Normans, (Norman:, Nourmands;, French:, ...</td>\n      <td>France</td>\n      <td>False</td>\n      <td>26</td>\n      <td>26</td>\n      <td>[France.]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56ddde6b9a695914005b9628</td>\n      <td>[In, what, country, is, Normandy, located?]</td>\n      <td>[The, Normans, (Norman:, Nourmands;, French:, ...</td>\n      <td>France</td>\n      <td>False</td>\n      <td>26</td>\n      <td>26</td>\n      <td>[France.]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56ddde6b9a695914005b9628</td>\n      <td>[In, what, country, is, Normandy, located?]</td>\n      <td>[The, Normans, (Norman:, Nourmands;, French:, ...</td>\n      <td>France</td>\n      <td>False</td>\n      <td>26</td>\n      <td>26</td>\n      <td>[France.]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56ddde6b9a695914005b9628</td>\n      <td>[In, what, country, is, Normandy, located?]</td>\n      <td>[The, Normans, (Norman:, Nourmands;, French:, ...</td>\n      <td>France</td>\n      <td>False</td>\n      <td>26</td>\n      <td>26</td>\n      <td>[France.]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>56ddde6b9a695914005b9629</td>\n      <td>[When, were, the, Normans, in, Normandy?]</td>\n      <td>[The, Normans, (Norman:, Nourmands;, French:, ...</td>\n      <td>10th and 11th centuries</td>\n      <td>False</td>\n      <td>14</td>\n      <td>17</td>\n      <td>[10th, and, 11th, centuries]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_encoded = encode_data(val_processed, tokenizer, MAX_SEQ_LEN, MAX_QN_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:24:38.256711Z","iopub.execute_input":"2023-08-04T11:24:38.257881Z","iopub.status.idle":"2023-08-04T11:27:22.692712Z","shell.execute_reply.started":"2023-08-04T11:24:38.257846Z","shell.execute_reply":"2023-08-04T11:27:22.691621Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 26232/26232 [02:44<00:00, 159.54it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creation of Dataloader\ninput_ids = torch.tensor([sample['ids'] for sample in val_encoded], dtype=torch.long)\ninput_mask = torch.tensor([sample['mask'] for sample in val_encoded], dtype=torch.long)\nsegment_ids = torch.tensor([sample['token_type_ids'] for sample in val_encoded], dtype=torch.long)\n\nstart_positions = torch.tensor([sample['start_pos'] for sample in val_encoded], dtype=torch.long)\nend_positions = torch.tensor([sample['end_pos'] for sample in val_encoded], dtype=torch.long)\n\nval_dataset = TensorDataset(input_ids, input_mask, segment_ids, start_positions, end_positions)\n\nval_params = {'batch_size': BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }    \n\nval_loader = DataLoader(val_dataset, **val_params)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:22.694181Z","iopub.execute_input":"2023-08-04T11:27:22.694607Z","iopub.status.idle":"2023-08-04T11:27:28.590789Z","shell.execute_reply.started":"2023-08-04T11:27:22.694572Z","shell.execute_reply":"2023-08-04T11:27:28.589809Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Evaluation Metrics for Prediction","metadata":{}},{"cell_type":"code","source":"# Functions to calculate Evaluation Metrics\ndef normalize_text(s):\n    \"\"\"Removing articles, punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(prediction, truth):\n    ''' Calculates the F1 score'''\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n\n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n\n    common_tokens = set(pred_tokens) & set(truth_tokens)\n\n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n\n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n\n    return 2 * (prec * rec) / (prec + rec)\n\ndef compute_exact_match(prediction, truth):\n    ''' Computes the exact match score '''\n    return int(normalize_text(prediction) == normalize_text(truth))","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:28.592108Z","iopub.execute_input":"2023-08-04T11:27:28.592467Z","iopub.status.idle":"2023-08-04T11:27:28.605621Z","shell.execute_reply.started":"2023-08-04T11:27:28.592433Z","shell.execute_reply":"2023-08-04T11:27:28.604641Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train(model, training_loader, optimizer):\n    ''' Training finetunes the ELECTRA model for Question Answering'''\n    print('Starting training...')\n    step = 0\n    counter = 0\n    loss_tracker = 0\n    em_score = 0\n    f1_score = 0\n    model.zero_grad()\n    model.train()\n    counter = 0\n    for data in tqdm(training_loader):\n        data = tuple(d.to(device) for d in data)\n        inputs = {'input_ids':     data[0],\n                'attention_mask':  data[1], \n                'token_type_ids':  data[2],  \n                'start_positions': data[3], \n                'end_positions':   data[4]}\n        outputs = model(**inputs)\n        loss = outputs[0]\n        loss.backward() # back propagation\n        optimizer.step()\n        model.zero_grad()\n        \n        starts = outputs[1]\n        ends = outputs[2]\n        target_starts = data[3]\n        target_ends = data[4]\n        \n        for i,(s,e) in enumerate(zip(starts, ends)):\n            start_pred = torch.argmax(s)\n            end_pred = torch.argmax(e)\n            predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][start_pred : end_pred+1]))\n            predicted_answer = predicted_answer if predicted_answer!= '[CLS]' else ''\n            actual_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][target_starts[i] : target_ends[i]+1]))\n            curr_em = compute_exact_match(predicted_answer, actual_answer)\n            curr_f1 = compute_f1(predicted_answer, actual_answer)\n            em_score += curr_em\n            f1_score += curr_f1\n            counter += 1\n                \n        loss_tracker += loss.item()\n        step += 1\n        if step % 1000 == 0:\n            print(\"Train loss: {}, Exact Match: {}, F1 Score: {}\".format(loss_tracker/step, em_score/counter, f1_score/counter))\n\n    return loss_tracker/step, em_score/counter, f1_score/counter\n\n\ndef validator(model, testing_loader):\n    ''' Performs Prediction of the Answers '''\n    print('Starting validation...')\n    model.eval()\n    preds = []\n    targs = []\n    pred_answers = []\n    target_answers = []\n    val_loss = 0\n    step = 0\n    with torch.no_grad():\n        for data in tqdm(testing_loader):\n            data = tuple(d.to(device) for d in data)\n            inputs = {'input_ids': data[0],\n                'attention_mask':  data[1], \n                'token_type_ids':  data[2],\n                'start_positions': data[3], \n                'end_positions':   data[4]}\n            output = model(**inputs)\n            valloss = output.loss\n            val_loss += valloss.item()\n            starts = output[1]\n            ends = output[2]\n            start_preds = []\n            end_preds = []\n            \n            target_starts = data[3]\n            target_ends = data[4]\n            step += 1\n\n            for s,e in zip(starts, ends):\n                start_pred = torch.argmax(s)\n                start_preds.append(start_pred)\n                end_pred = torch.argmax(e)\n                end_preds.append(end_pred)\n\n            for i, (s,e) in enumerate(zip(start_preds, target_ends)):\n                preds.append((s,e))\n                predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                pred_answers.append(predicted_answer)\n            \n            for i, (s,e) in enumerate(zip(target_starts, target_ends)):\n                targs.append((s,e))\n                actual_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][s : e+1]))\n                target_answers.append(actual_answer)\n                \n        em_score = 0\n        f1_score = 0\n        pred_answers = [item if item != '[CLS]' else '' for item in pred_answers]\n        for predicted_ans, target_ans in zip(pred_answers, target_answers):\n            em_score += compute_exact_match(predicted_ans, target_ans)\n            f1_score += compute_f1(predicted_ans, target_ans)\n        em_score /= len(pred_answers)\n        f1_score /= len(pred_answers)\n        val_loss /= step\n    return pred_answers, target_answers, val_loss, em_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:28.607056Z","iopub.execute_input":"2023-08-04T11:27:28.607591Z","iopub.status.idle":"2023-08-04T11:27:28.630886Z","shell.execute_reply.started":"2023-08-04T11:27:28.607549Z","shell.execute_reply":"2023-08-04T11:27:28.629951Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Finetuning the ELECTRA Model","metadata":{}},{"cell_type":"code","source":"model = ElectraForQuestionAnswering.from_pretrained('google/electra-base-discriminator')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:28.650363Z","iopub.execute_input":"2023-08-04T11:27:28.650920Z","iopub.status.idle":"2023-08-04T11:27:37.165220Z","shell.execute_reply.started":"2023-08-04T11:27:28.650869Z","shell.execute_reply":"2023-08-04T11:27:37.164148Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e23f96009e846a793a7d491b833d892"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"ElectraForQuestionAnswering(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:37.166812Z","iopub.execute_input":"2023-08-04T11:27:37.167198Z","iopub.status.idle":"2023-08-04T11:27:37.176170Z","shell.execute_reply.started":"2023-08-04T11:27:37.167165Z","shell.execute_reply":"2023-08-04T11:27:37.172249Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for epoch in range(NO_EPOCHS): #TRAINING\n    loss, em, f1 = train(model, training_loader, optimizer)\n    print(f'Epoch: {epoch}, Loss:  {loss}, Em: {em}, F1: {f1}') \n    pred_answers, target_answers, val_loss, em_score, f1_score = validator(model, val_loader)\n    print(f'Loss:  {val_loss}, Em: {em_score}, F1: {f1_score}')\n    torch.save(model, '/kaggle/working/fine_tuned_electra'+str(epoch)+'.model')","metadata":{"execution":{"iopub.status.busy":"2023-08-04T11:27:37.177403Z","iopub.execute_input":"2023-08-04T11:27:37.177986Z","iopub.status.idle":"2023-08-04T17:43:16.897155Z","shell.execute_reply.started":"2023-08-04T11:27:37.177951Z","shell.execute_reply":"2023-08-04T17:43:16.896108Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 1000/16278 [07:16<1:50:29,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 2.257298290669918, Exact Match: 0.3325, F1 Score: 0.4092865794492176\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 2000/16278 [14:29<1:42:56,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.9815798905193807, Exact Match: 0.3844375, F1 Score: 0.4665724131950755\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 3000/16278 [21:43<1:35:43,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.8474137058754763, Exact Match: 0.4103333333333333, F1 Score: 0.4941281084666153\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 4000/16278 [28:57<1:29:42,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.7649945277273655, Exact Match: 0.42790625, F1 Score: 0.5119062245614254\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 5000/16278 [36:10<1:21:28,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.708784855657816, Exact Match: 0.439575, F1 Score: 0.5254793234234655\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 6000/16278 [43:23<1:14:33,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.6672925110707681, Exact Match: 0.448, F1 Score: 0.5341009610376782\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 7000/16278 [50:36<1:06:48,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.6337495823970862, Exact Match: 0.45626785714285717, F1 Score: 0.5421724972009891\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 8000/16278 [57:50<59:46,  2.31it/s]  ","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.6059015740565956, Exact Match: 0.46234375, F1 Score: 0.5474015382132634\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 9000/16278 [1:05:03<52:46,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.579382317248318, Exact Match: 0.46768055555555554, F1 Score: 0.5531772490528508\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 10000/16278 [1:12:16<45:11,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.5599040645092725, Exact Match: 0.471925, F1 Score: 0.5576545896247355\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 11000/16278 [1:19:30<38:13,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.5442760518518361, Exact Match: 0.4753181818181818, F1 Score: 0.5609560384303799\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▎  | 12000/16278 [1:26:43<30:45,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.530880736236771, Exact Match: 0.47789583333333335, F1 Score: 0.5633761085498853\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 13000/16278 [1:33:57<23:33,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.519468747228384, Exact Match: 0.48057692307692307, F1 Score: 0.5661749456182481\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 14000/16278 [1:41:10<16:25,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.5067433749330896, Exact Match: 0.483875, F1 Score: 0.56943067560867\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 15000/16278 [1:48:23<09:17,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.4969059985856215, Exact Match: 0.485625, F1 Score: 0.5710138322838506\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 16000/16278 [1:55:37<02:00,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.4850976998731493, Exact Match: 0.4880234375, F1 Score: 0.5736287042144831\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16278/16278 [1:57:37<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  1.4822412325381236, Em: 0.4887226706190436, F1: 0.5743957333285833\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3279/3279 [07:33<00:00,  7.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss:  1.4093137712568071, Em: 0.7663921927416896, F1: 0.8419866418392424\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 1000/16278 [07:13<1:50:18,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1052763902246951, Exact Match: 0.56775, F1 Score: 0.645183872684568\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 2000/16278 [14:26<1:42:40,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1047375701665878, Exact Match: 0.5724375, F1 Score: 0.649411246144707\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 3000/16278 [21:40<1:35:34,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1361906071454286, Exact Match: 0.5685833333333333, F1 Score: 0.6474149987688114\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 4000/16278 [28:53<1:29:07,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1322428409792482, Exact Match: 0.5703125, F1 Score: 0.6501964260225642\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 5000/16278 [36:07<1:21:08,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1323554707497359, Exact Match: 0.568525, F1 Score: 0.6479610750298747\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 6000/16278 [43:20<1:14:18,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1285220480983456, Exact Match: 0.5688541666666667, F1 Score: 0.6484377947147796\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 7000/16278 [50:33<1:07:04,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1284509751221963, Exact Match: 0.567375, F1 Score: 0.648084978812085\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 8000/16278 [57:46<59:54,  2.30it/s]  ","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1302967326249926, Exact Match: 0.5660625, F1 Score: 0.6468249753119036\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 9000/16278 [1:05:00<52:27,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.131004794943664, Exact Match: 0.5665138888888889, F1 Score: 0.6473136346329298\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 10000/16278 [1:12:13<45:06,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1310025657832623, Exact Match: 0.565375, F1 Score: 0.6460901106845375\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 11000/16278 [1:19:26<38:03,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1322819618745283, Exact Match: 0.5651477272727272, F1 Score: 0.6457723348756903\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▎  | 12000/16278 [1:26:40<30:49,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.13250890213872, Exact Match: 0.5649166666666666, F1 Score: 0.6457116982967629\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 13000/16278 [1:33:53<23:35,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1322673233850644, Exact Match: 0.5647211538461538, F1 Score: 0.6456518278590764\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 14000/16278 [1:41:07<16:29,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.133531950847379, Exact Match: 0.5644017857142857, F1 Score: 0.6454200433270476\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 15000/16278 [1:48:20<09:15,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1327028562376897, Exact Match: 0.5644333333333333, F1 Score: 0.6455324341337922\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 16000/16278 [1:55:34<02:00,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.1334482663860546, Exact Match: 0.563609375, F1 Score: 0.6447861967551006\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16278/16278 [1:57:34<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss:  1.1339856157577555, Em: 0.5635208920494252, F1: 0.6446315090737822\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3279/3279 [07:34<00:00,  7.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss:  1.4790602469633323, Em: 0.761741384568466, F1: 0.8359031776951983\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 1000/16278 [07:13<1:50:28,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9146091934889555, Exact Match: 0.61925, F1 Score: 0.6927773235334815\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 2000/16278 [14:26<1:43:16,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9095034807771444, Exact Match: 0.618625, F1 Score: 0.6933275464137119\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 3000/16278 [21:40<1:35:40,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9188480047533909, Exact Match: 0.616625, F1 Score: 0.6907486916126169\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▍       | 4000/16278 [28:53<1:29:04,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.925996621368453, Exact Match: 0.6133125, F1 Score: 0.6874757712481501\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 5000/16278 [36:07<1:21:16,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9318680788710714, Exact Match: 0.612325, F1 Score: 0.6861527671598062\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 6000/16278 [43:20<1:14:08,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9393819619653125, Exact Match: 0.6118541666666667, F1 Score: 0.6851310435379908\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 7000/16278 [50:34<1:07:03,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.942881225902055, Exact Match: 0.6103392857142858, F1 Score: 0.6835775419174301\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 8000/16278 [57:47<1:00:28,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9478172356644645, Exact Match: 0.60875, F1 Score: 0.6826002224586872\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 9000/16278 [1:05:01<52:32,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9511225775140855, Exact Match: 0.6081388888888889, F1 Score: 0.6825143447709032\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████▏   | 10000/16278 [1:12:14<45:28,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9561769568048417, Exact Match: 0.6072375, F1 Score: 0.681953966812798\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 11000/16278 [1:19:28<38:23,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9596717871136285, Exact Match: 0.6058977272727273, F1 Score: 0.6809765786195557\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▎  | 12000/16278 [1:26:42<30:53,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9613804509819796, Exact Match: 0.6050104166666667, F1 Score: 0.6800166796546042\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 13000/16278 [1:33:55<23:42,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9653271642900431, Exact Match: 0.6042019230769231, F1 Score: 0.6791418387782584\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 14000/16278 [1:41:09<16:28,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.968234985084406, Exact Match: 0.6032410714285714, F1 Score: 0.6784162689062176\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 15000/16278 [1:48:23<09:15,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9717069536139568, Exact Match: 0.6022, F1 Score: 0.6773384063470559\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 16000/16278 [1:55:36<02:00,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.9730943836579099, Exact Match: 0.6016015625, F1 Score: 0.6770966638214976\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16278/16278 [1:57:36<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss:  0.9740563340904814, Em: 0.6014959644286076, F1: 0.6770540627827345\nStarting validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3279/3279 [07:33<00:00,  7.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss:  1.4832015226402817, Em: 0.7652485513876182, F1: 0.8431609297202848\n","output_type":"stream"}]}]}